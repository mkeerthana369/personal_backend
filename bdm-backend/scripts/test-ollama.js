// ============================================
// FILE: scripts/test-ollama.js
// Test Ollama connectivity
// ============================================

const axios = require('axios');
require('dotenv').config();

async function testOllama() {
  const host = process.env.OLLAMA_HOST || 'http://localhost:11434';
  const model = process.env.OLLAMA_MODEL || 'llama3.2:3b';
  
  console.log('ğŸ¦™ Testing Ollama connection...');
  console.log(`ğŸ“ Host: ${host}`);
  console.log(`ğŸ¤– Model: ${model}`);
  console.log('');
  
  try {
    // Test 1: Check if Ollama is running
    console.log('1ï¸âƒ£ Checking if Ollama is running...');
    const tagsResponse = await axios.get(`${host}/api/tags`, { timeout: 5000 });
    console.log('   âœ… Ollama is running');
    
    // Test 2: Check if model exists
    const models = tagsResponse.data.models || [];
    const modelExists = models.some(m => m.name === model);
    
    if (modelExists) {
      console.log(`   âœ… Model "${model}" is installed`);
    } else {
      console.log(`   âš ï¸  Model "${model}" not found`);
      console.log(`   ğŸ“¥ Available models:`);
      models.forEach(m => console.log(`      - ${m.name}`));
      console.log('');
      console.log(`   ğŸ’¡ To install: ollama pull ${model}`);
      return;
    }
    
    // Test 3: Test generation
    console.log('');
    console.log('2ï¸âƒ£ Testing generation...');
    const testPrompt = 'Say "Hello from Ollama!" and nothing else.';
    
    const generateResponse = await axios.post(
      `${host}/api/generate`,
      {
        model: model,
        prompt: testPrompt,
        stream: false,
        options: {
          temperature: 0.7,
          num_predict: 50
        }
      },
      { timeout: 30000 }
    );
    
    console.log('   âœ… Generation successful!');
    console.log(`   ğŸ“ Response: ${generateResponse.data.response.substring(0, 100)}`);
    console.log('');
    console.log('ğŸ‰ All tests passed! Ollama is ready to use.');
    console.log('');
    console.log('ğŸ“š Next steps:');
    console.log('   1. Ensure USE_OLLAMA=true in .env');
    console.log('   2. Run: npm run dev');
    console.log('   3. Try: POST http://localhost:5000/api/clauses/generate-ai');
    
  } catch (error) {
    console.error('');
    console.error('âŒ Test failed:', error.message);
    console.error('');
    console.error('ğŸ’¡ Troubleshooting:');
    
    if (error.code === 'ECONNREFUSED') {
      console.error('   âš ï¸  Cannot connect to Ollama');
      console.error('   ğŸ“¥ Install: winget install Ollama.Ollama');
      console.error('   ğŸ”„ Then restart your system');
      console.error('   âœ… Verify: ollama --version');
    } else if (error.response?.status === 404) {
      console.error('   âš ï¸  Model not found');
      console.error(`   ğŸ“¥ Install model: ollama pull ${model}`);
    } else {
      console.error('   âš ï¸  Unexpected error');
      console.error(`   ğŸ“‹ Details: ${error.message}`);
    }
    
    console.error('');
    console.error('   ğŸ“– Check: https://ollama.com/download');
  }
}

testOllama();